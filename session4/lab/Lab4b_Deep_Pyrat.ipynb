{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Artificial Intelligence - Supervised Learning lab Session Part b\n",
    "--\n",
    "At the end of this session, you will be able to : \n",
    "- Learn the basics of pytorch in this [tutorial](Pytorch_tutorial.ipynb) (this can be done in parallel that the previous step if you work by groups of two)\n",
    "- Apply supervised learning on PyRat datasets and traini a classifer to predict the next movement to play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tqdm package is useful to visualize progress with long computations. \n",
    "# Install it using pip. \n",
    "import tqdm\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import inspect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch tutorial\n",
    "--\n",
    "Go [here](Lab4a_Pytorch_tutorial.ipynb) and perform the pytorch tutorial before moving to part b (this one). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing PyRat using Machine Learning by training a classifier to predict the next movement to play (or - Supervised Baseline for Pyrat Challenge)\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we now take a step further with respect to Lab_1a and try to predict the next movement to play given a maze configuration. We therefore need to generate a new training dataset (X=canvas, y=next movement) with pyrat games to train a model. In particular we will train a deep neural network. Note that you will have to define a model in pytorch, so you have to do the pytorch tutorial first. \n",
    "\n",
    "The canvas here represents the state of the game and it corresponds to the vector that will be used to train the classifier. As we want to predict a next move, the canvas is twice the size of the maze and is centered on the player, so that we create a translation invariance.\n",
    "\n",
    "Have a look at the file `generate_SL_dataset.py`. It generates a dataset (`SupervisedLearning_experience.pt`) for training a classifier to predict the next move given a game configuration. The canvas (state of the game) is generated by the function `build_state` and is stored in memory together with the corresponding action at each turn of the game. `build_state` outputs a one layer canvas, but you can define other layers to put more information on the play (e.g. the location of the opponent could be put in a second layer). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the pyrat_dataset that was stored as a .pt file by the generate_SL_dataset.py script. \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "BATCH_SIZE=50\n",
    "N_EPOCHS=20\n",
    "\n",
    "## CELL TO BE COMPLETED ##\n",
    "SL_dataset = # path to Supervised Learning dataset \n",
    "data=torch.load(SL_dataset)\n",
    "\n",
    "#how many examples?\n",
    "\n",
    "\n",
    "# an example of the canvas (corresponding to first game) is\n",
    "x = data[0][\"state\"]\n",
    "print(x.shape)\n",
    "# the corresponding label (one-hot encoded 'action') \n",
    "y = data[0][\"action\"]\n",
    "print(y)\n",
    "\n",
    "# maze and canvas size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch (data, batch=BATCH_SIZE):\n",
    "\n",
    "    \"\"\"\n",
    "        This function builds batches from the dataset to train the model on.\n",
    "        Each batch is a pair (data, target), where each element has batch size as first dimension.\n",
    "        In:\n",
    "            \n",
    "            * experience:       List of experience situations encountered across games.\n",
    "                \n",
    "        Out:\n",
    "            * data:    Batch of data.\n",
    "            * targets: Targets associated with the sampled data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get indices\n",
    "    batch_size = min(batch, len(data))\n",
    "    indices = random.sample(range(len(data)), batch_size)\n",
    "\n",
    "    # Create the batch\n",
    "    X = torch.zeros(batch_size, data[0][\"state\"].shape[0]*data[0][\"state\"].shape[1])\n",
    "    y = torch.zeros(batch_size,dtype=torch.int64)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        #Â Data is the sampled state\n",
    "        X[i] = torch.flatten(data[indices[i]][\"state\"])  # flatten canvas (input to model)   \n",
    "        y[i] = data[indices[i]][\"action\"]\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now you have to train a classifier using supervised learning and evaluate it's performance. \n",
    "### Let's try a neural network.\n",
    "\n",
    "## Split your data into x_train, x_test, y_train, y_test.\n",
    "\n",
    "n = int(len(data) * 80/100)  # number of examples in the train set\n",
    "train_data=data[:n]\n",
    "test_data=data[n:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a neural network with two hidden layers. In pytorch, this correspond to only adding two layers of type \"Linear\".\n",
    "## You need to make sure that the size of the input of the first layer correspond to the width of your X vector. \n",
    "\n",
    "## CELL TO BE COMPLETED ##\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 512)\n",
    "        ## ADD LAYERS HERE ##\n",
    "       \n",
    "    def forward(self, x):\n",
    "        ## ADD FORWARD FUNCTIONS HERE ##\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate your model spefying the in_features (size of a \"flattened\" input)\n",
    "\n",
    "net=Net(train_data[0]['state'].shape[0]*train_data[0]['state'].shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CELL TO BE COMPLETED ##\n",
    "## Define a loss function and optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## CELL TO BE COMPLETED ##\n",
    "## Train the network\n",
    "n_batch=len(data)//BATCH_SIZE\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss = 0\n",
    "    # get the inputs\n",
    "    for b in range(n_batch):\n",
    "        inputs,labels = make_batch(train_data)\n",
    "        # ZERO THE PARAMETERs GRADIENT\n",
    "       \n",
    "        # FORWARD + BACKWARD + OPTIMIZE\n",
    "        \n",
    "        \n",
    "        # statistics\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('[%d] loss: %.3f' % (epoch + 1, running_loss/n_batch ))\n",
    "    \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check performances\n",
    "## Training accuracy\n",
    "correct = 0\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs,labels = make_batch(train_data, batch=len(train_data))\n",
    "    outputs = net(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Training accuracy of the network: %d %%' % (100 * correct / len(train_data)))\n",
    "        \n",
    "## Test accuracy\n",
    "correct = 0\n",
    "total = len(test_data)\n",
    "with torch.no_grad():\n",
    "    inputs,labels = make_batch(test_data, batch=len(test_data))\n",
    "    outputs = net(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Test accuracy of the network: %d %%' % (100 * correct / len(test_data)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the weights\n",
    "\n",
    "torch.save(net.state_dict(), 'AI/trained_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks on training a NN \n",
    "\n",
    "If the training accuracy is about 20%, it means the network predicts the result as good as chance (5 possible choices: North, South, East, West, Nothing).\n",
    "\n",
    "When you train a neural network, you have to analyze your results. If, after the training, your training accuracy is far from 100%, your network is underfitting (high bias). Try to train the network longer (more epochs, bigger/smaller learning rate, batch size). Or, define a bigger network (more hidden layers, bigger out_features).  If, your test accuracy is far from your training accuracy, your network is overfitting (high variance). Try to regularize your optimization (look at L2 regularization, weight decay, drop out, early stopping...).\n",
    "Try to use more data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test in PyRat\n",
    "\n",
    "Now, it's time to test if your AI is able to beat an opponent. Open the `supervised_pyrat_player.py` file, and update the `TRAINED_MODEL_PATH` constant to set the path to the classifier you want to use. Also modify/complete the code corresponding to the parts\n",
    "\n",
    "########\n",
    "#### TODO ####   \n",
    "########\n",
    "\n",
    "Then run the `supervised_player.py` with the following command, changing the needed parameters. Make sure you use the same settings (width/height/number of cheeeses) as during training:\n",
    "\n",
    "<br />`python supervised_pyrat_player.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well does the trained classifier play against the greedy? \n",
    "\n",
    "Now it's up to you to explore other possibilities to make a better player. A few starting points: \n",
    "- Change the \"canvas\" to add more information, such as the position of the other player. \n",
    "- Find more clever strategies to cross validate training, in order to enable a better estimate of generalization\n",
    "- Work on simpler versions of the problem (smaller maze, less cheese, ..) , to develop a better understanding of learning.\n",
    "- Generate datasets using another algorithm than the greedy (eg, a variant that surely beats the greedy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CELL TO BE COMPLETED ##\n",
    "# You can use the simulations.run_several_games function to test the performances of your trained model vs a greedy or a random player\n",
    "\n",
    "import sys     # These lines correct a bug occuring in Notebooks.\n",
    "sys.argv=['']  # It's not perfect, but it works.\n",
    "\n",
    "import os\n",
    "lab_commons_path = os.path.join(os.getcwd(), \"..\", \"..\")\n",
    "if lab_commons_path not in sys.path:\n",
    "    sys.path.append(lab_commons_path)\n",
    "\n",
    "import lab_commons.make_2_player_matches as simulations\n",
    "import lab_commons.AI.greedy as greedy_player\n",
    "import lab_commons.AI.random as random_player\n",
    "import supervised_pyrat_player\n",
    "\n",
    "program_1 = supervised_pyrat_player \n",
    "program_2 = greedy_player\n",
    "program_3 = random_player\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
